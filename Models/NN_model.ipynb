{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch import optim\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.metrics import r2_score, mean_squared_error\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('train.csv')\n",
    "df_raw = pd.read_csv('train.csv')\n",
    "\n",
    "# drop unnecessary columns\n",
    "df = df.drop(columns=['weather_temperature', 'weather_wind_mph', 'weather_humidity', 'weather_detail', 'stadium'])\n",
    "df.head()\n",
    "\n",
    "# adding total score columns\n",
    "df['total_score'] = df['score_home'] + df['score_away']\n",
    "\n",
    "\n",
    "# add column that represented current record for each team before each game of a season\n",
    "# ensure games are sorted chronologically within each season\n",
    "df[\"datetime\"] = pd.to_datetime(df[\"schedule_date\"])\n",
    "df = df.sort_values([\"schedule_season\", \"datetime\"]).reset_index(drop=True)\n",
    "\n",
    "# make output lists\n",
    "home_records = []\n",
    "away_records = []\n",
    "\n",
    "# make dictionaries to track each team's W-L-T within the current season\n",
    "team_wins = {}\n",
    "team_losses = {}\n",
    "team_ties = {}\n",
    "\n",
    "current_season = None\n",
    "\n",
    "for i, row in df.iterrows():\n",
    "    season = row[\"schedule_season\"]\n",
    "    home = row[\"team_home\"]\n",
    "    away = row[\"team_away\"]\n",
    "    home_score = row[\"score_home\"]\n",
    "    away_score = row[\"score_away\"]\n",
    "   \n",
    "    # new season,  reset all\n",
    "    if season != current_season:\n",
    "        team_wins = {}\n",
    "        team_losses = {}\n",
    "        team_ties = {}\n",
    "        current_season = season\n",
    "\n",
    "    # initialize teams for this season if needed\n",
    "    for team in [home, away]:\n",
    "        if team not in team_wins:\n",
    "            team_wins[team] = 0\n",
    "            team_losses[team] = 0\n",
    "            team_ties[team] = 0\n",
    "\n",
    "    # add current record before the game\n",
    "    home_records.append(\n",
    "        f\"{team_wins[home]}-{team_losses[home]}-{team_ties[home]}\"\n",
    "    )\n",
    "    away_records.append(\n",
    "        f\"{team_wins[away]}-{team_losses[away]}-{team_ties[away]}\"\n",
    "    )\n",
    "\n",
    "    # update records after the game\n",
    "    if home_score > away_score:\n",
    "        team_wins[home] += 1\n",
    "        team_losses[away] += 1\n",
    "    elif away_score > home_score:\n",
    "        team_wins[away] += 1\n",
    "        team_losses[home] += 1\n",
    "    else:\n",
    "        # tie\n",
    "        team_ties[home] += 1\n",
    "        team_ties[away] += 1\n",
    "\n",
    "# add results to dataframe\n",
    "df[\"home_team_record\"] = home_records\n",
    "df[\"away_team_record\"] = away_records\n",
    "\n",
    "\n",
    "# make individual columns for wins, losses, and ties\n",
    "df['home_wins'] = df['home_team_record'].apply(lambda x: int(x.split('-')[0]))\n",
    "df['home_losses'] = df['home_team_record'].apply(lambda x: int(x.split('-')[1]))\n",
    "df['home_ties'] = df['home_team_record'].apply(lambda x: int(x.split('-')[2]))\n",
    "df['away_wins'] = df['away_team_record'].apply(lambda x: int(x.split('-')[0]))\n",
    "df['away_losses'] = df['away_team_record'].apply(lambda x: int(x.split('-')[1]))\n",
    "df['away_ties'] = df['away_team_record'].apply(lambda x: int(x.split('-')[2]))\n",
    "\n",
    "\n",
    "# filter games that have already been recorded, no scheduled games\n",
    "df = df[df[\"datetime\"] <= \"2025-11-04\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute each team's average score per season\n",
    "team_season_avg = (\n",
    "    df.groupby([\"team_home\", \"schedule_season\"])[\"score_home\"].mean().reset_index()\n",
    ")\n",
    "team_season_avg.columns = [\"team\", \"season\", \"avg_score\"]\n",
    "\n",
    "# also include away team scoring\n",
    "team_season_avg_away = (\n",
    "    df.groupby([\"team_away\", \"schedule_season\"])[\"score_away\"].mean().reset_index()\n",
    ")\n",
    "team_season_avg_away.columns = [\"team\", \"season\", \"avg_score\"]\n",
    "\n",
    "# combine home + away scoring for a true team season average\n",
    "team_season_avg = pd.concat([team_season_avg, team_season_avg_away])\n",
    "team_season_avg = team_season_avg.groupby([\"team\", \"season\"])[\"avg_score\"].mean().reset_index()\n",
    "\n",
    "# shift averages to represent previous season\n",
    "team_season_avg[\"prev_season\"] = team_season_avg[\"season\"] + 1\n",
    "\n",
    "# prev_season avg is used in the next year's games\n",
    "team_prev = team_season_avg[[\"team\", \"prev_season\", \"avg_score\"]]\n",
    "team_prev.columns = [\"team\", \"schedule_season\", \"prev_season_avg\"]\n",
    "\n",
    "# merge into main df\n",
    "df = df.merge(team_prev, left_on=[\"team_home\", \"schedule_season\"], right_on=[\"team\", \"schedule_season\"], how=\"left\")\n",
    "df.rename(columns={\"prev_season_avg\": \"home_prev_avg\"}, inplace=True)\n",
    "df = df.drop(columns=[\"team\"])\n",
    "\n",
    "df = df.merge(team_prev, left_on=[\"team_away\", \"schedule_season\"], right_on=[\"team\", \"schedule_season\"], how=\"left\")\n",
    "df.rename(columns={\"prev_season_avg\": \"away_prev_avg\"}, inplace=True)\n",
    "df = df.drop(columns=[\"team\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating rolling avgs within each season\n",
    "\n",
    "# separate into home, away dfs\n",
    "home = df[[\"schedule_season\", \"datetime\", \"team_home\", \"score_home\", \"score_away\"]].rename(\n",
    "    columns={\"team_home\": \"team\", \"score_home\": \"points_scored\", \"score_away\": \"points_allowed\"}\n",
    ")\n",
    "\n",
    "away = df[[\"schedule_season\", \"datetime\", \"team_away\", \"score_away\", \"score_home\"]].rename(\n",
    "    columns={\"team_away\": \"team\", \"score_away\": \"points_scored\", \"score_home\": \"points_allowed\"}\n",
    ")\n",
    "\n",
    "# long df, duplicate games, sort by team, season, date\n",
    "long_df = pd.concat([home, away])\n",
    "long_df = long_df.sort_values([\"team\", \"schedule_season\", \"datetime\"]).reset_index(drop=True)\n",
    "\n",
    "groups = long_df.groupby([\"team\", \"schedule_season\"])\n",
    "\n",
    "# compute rolling averages\n",
    "long_df[\"rolling_scored\"] = groups[\"points_scored\"].transform(\n",
    "    lambda s: s.shift().expanding().mean()\n",
    ")\n",
    "\n",
    "long_df[\"rolling_allowed\"] = groups[\"points_allowed\"].transform(\n",
    "    lambda s: s.shift().expanding().mean()\n",
    ")\n",
    "\n",
    "# merge back into original df\n",
    "df = df.merge(\n",
    "    long_df[[\"team\", \"schedule_season\", \"datetime\", \"rolling_scored\", \"rolling_allowed\"]],\n",
    "    left_on=[\"team_home\", \"schedule_season\", \"datetime\"],\n",
    "    right_on=[\"team\", \"schedule_season\", \"datetime\"],\n",
    "    how=\"left\"\n",
    ").rename(\n",
    "    columns={\n",
    "        \"rolling_scored\": \"home_rolling_scored\",\n",
    "        \"rolling_allowed\": \"home_rolling_allowed\"\n",
    "    }\n",
    ").drop(columns=[\"team\"])\n",
    "\n",
    "\n",
    "df = df.merge(\n",
    "    long_df[[\"team\", \"schedule_season\", \"datetime\", \"rolling_scored\", \"rolling_allowed\"]],\n",
    "    left_on=[\"team_away\", \"schedule_season\", \"datetime\"],\n",
    "    right_on=[\"team\", \"schedule_season\", \"datetime\"],\n",
    "    how=\"left\"\n",
    ").rename(\n",
    "    columns={\n",
    "        \"rolling_scored\": \"away_rolling_scored\",\n",
    "        \"rolling_allowed\": \"away_rolling_allowed\"\n",
    "    }\n",
    ").drop(columns=[\"team\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dropping variables that aren't informative to our model\n",
    "df_filtered = df.drop(columns=['datetime', 'stadium_neutral', 'home_team_record', 'away_team_record',\n",
    "                       'schedule_date', 'team_favorite_id', 'team_home', 'team_away', 'schedule_week'])\n",
    "df_filtered = df_filtered.dropna(axis = 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# splitting into x and y\n",
    "X = df_filtered.drop(columns=['total_score', 'over_under_line', 'score_home', 'score_away'])\n",
    "y = df_filtered['total_score']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train/test splitting\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size= 0.2, random_state = 123)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Learning rate: 0.0005, Hidden units: 16, Epochs: 150, MSE: 188.854, R^2: 0.021\n",
      "Learning rate: 0.0005, Hidden units: 16, Epochs: 300, MSE: 189.209, R^2: 0.019\n",
      "Learning rate: 0.0005, Hidden units: 16, Epochs: 450, MSE: 190.775, R^2: 0.011\n",
      "Learning rate: 0.0005, Hidden units: 32, Epochs: 150, MSE: 189.158, R^2: 0.019\n",
      "Learning rate: 0.0005, Hidden units: 32, Epochs: 300, MSE: 193.155, R^2: -0.001\n",
      "Learning rate: 0.0005, Hidden units: 32, Epochs: 450, MSE: 195.067, R^2: -0.011\n",
      "Learning rate: 0.0005, Hidden units: 64, Epochs: 150, MSE: 192.028, R^2: 0.004\n",
      "Learning rate: 0.0005, Hidden units: 64, Epochs: 300, MSE: 202.265, R^2: -0.049\n",
      "Learning rate: 0.0005, Hidden units: 64, Epochs: 450, MSE: 213.55, R^2: -0.107\n",
      "Learning rate: 0.001, Hidden units: 16, Epochs: 150, MSE: 188.783, R^2: 0.021\n",
      "Learning rate: 0.001, Hidden units: 16, Epochs: 300, MSE: 190.955, R^2: 0.01\n",
      "Learning rate: 0.001, Hidden units: 16, Epochs: 450, MSE: 194.446, R^2: -0.007\n",
      "Learning rate: 0.001, Hidden units: 32, Epochs: 150, MSE: 194.305, R^2: -0.007\n",
      "Learning rate: 0.001, Hidden units: 32, Epochs: 300, MSE: 200.785, R^2: -0.04\n",
      "Learning rate: 0.001, Hidden units: 32, Epochs: 450, MSE: 204.245, R^2: -0.058\n",
      "Learning rate: 0.001, Hidden units: 64, Epochs: 150, MSE: 200.986, R^2: -0.042\n",
      "Learning rate: 0.001, Hidden units: 64, Epochs: 300, MSE: 218.641, R^2: -0.133\n",
      "Learning rate: 0.001, Hidden units: 64, Epochs: 450, MSE: 244.683, R^2: -0.27\n",
      "Learning rate: 0.005, Hidden units: 16, Epochs: 150, MSE: 192.587, R^2: 0.001\n",
      "Learning rate: 0.005, Hidden units: 16, Epochs: 300, MSE: 194.656, R^2: -0.008\n",
      "Learning rate: 0.005, Hidden units: 16, Epochs: 450, MSE: 200.598, R^2: -0.041\n",
      "Learning rate: 0.005, Hidden units: 32, Epochs: 150, MSE: 195.509, R^2: -0.012\n",
      "Learning rate: 0.005, Hidden units: 32, Epochs: 300, MSE: 204.256, R^2: -0.058\n",
      "Learning rate: 0.005, Hidden units: 32, Epochs: 450, MSE: 218.079, R^2: -0.129\n",
      "Learning rate: 0.005, Hidden units: 64, Epochs: 150, MSE: 209.374, R^2: -0.085\n",
      "Learning rate: 0.005, Hidden units: 64, Epochs: 300, MSE: 256.171, R^2: -0.328\n",
      "Learning rate: 0.005, Hidden units: 64, Epochs: 450, MSE: 310.058, R^2: -0.608\n"
     ]
    }
   ],
   "source": [
    "class MyModel(nn.Module):\n",
    "    def __init__(self, h):\n",
    "        super().__init__()\n",
    "        l1 = nn.Linear(15, h)\n",
    "        a1 = nn.ReLU()\n",
    "        l2 = nn.Linear(h,h//2)\n",
    "        a2 = nn.ReLU()\n",
    "        l3 = nn.Linear(h//2,1)\n",
    "        l = [l1, a1, l2, a2, l3]\n",
    "        self.module_list = nn.ModuleList(l)\n",
    "    \n",
    "    def forward(self, X):\n",
    "        for f in self.module_list:\n",
    "            X = f(X)\n",
    "        return X\n",
    "\n",
    "torch.manual_seed(42)\n",
    "np.random.seed(42)\n",
    "\n",
    "k_folds = 5\n",
    "learning_rates = [0.0005, 0.001, 0.005]\n",
    "hidden_sizes = [16, 32, 64]\n",
    "epochs_options = [150, 300, 450]\n",
    "batch_size = 32\n",
    "\n",
    "\n",
    "def run_kfold(lr, h, epochs):\n",
    "    n_epochs = epochs\n",
    "    learning_rate = lr\n",
    "    hidden_size = h\n",
    "    kf = KFold(n_splits=k_folds, shuffle=True, random_state=42)\n",
    "\n",
    "\n",
    "    fold_mse = []\n",
    "    fold_r2 = []\n",
    "\n",
    "    for fold, (train_index, val_index) in enumerate(kf.split(X)):\n",
    "\n",
    "        X_np = X.values\n",
    "        y_np = y.values\n",
    "        X_train_fold, X_val_fold = X_np[train_index], X_np[val_index]\n",
    "        y_train_fold, y_val_fold = y_np[train_index], y_np[val_index]\n",
    "\n",
    "        scaler = StandardScaler()\n",
    "        X_train_fold = scaler.fit_transform(X_train_fold)\n",
    "        X_val_fold = scaler.transform(X_val_fold)\n",
    "\n",
    "        X_train_tensor = torch.tensor(X_train_fold, dtype=torch.float32)\n",
    "        y_train_tensor = torch.tensor(y_train_fold, dtype=torch.float32).reshape(-1, 1)\n",
    "        X_val_tensor = torch.tensor(X_val_fold, dtype=torch.float32)\n",
    "        y_val_tensor = torch.tensor(y_val_fold, dtype=torch.float32).reshape(-1, 1)\n",
    "\n",
    "        train_dataset = TensorDataset(X_train_tensor, y_train_tensor)\n",
    "        train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "        model = MyModel(hidden_size)\n",
    "\n",
    "        optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
    "        cost_function = nn.MSELoss()\n",
    "\n",
    "        for epoch in range(n_epochs):\n",
    "            model.train()\n",
    "            for x_batch, y_batch in train_loader:\n",
    "                optimizer.zero_grad()\n",
    "                y_pred = model(x_batch)\n",
    "                cost = cost_function(y_pred, y_batch)\n",
    "                cost.backward()\n",
    "                optimizer.step()\n",
    "\n",
    "        model.eval()\n",
    "        with torch.no_grad():\n",
    "            val_pred = model(X_val_tensor)\n",
    "            mse = cost_function(val_pred, y_val_tensor).item()\n",
    "            r2 = r2_score(y_val_tensor.numpy(), val_pred.numpy())\n",
    "\n",
    "        fold_mse.append(mse)\n",
    "        fold_r2.append(r2)\n",
    "    return np.mean(fold_mse), np.mean(fold_r2)\n",
    "\n",
    "\n",
    "for lr in learning_rates:\n",
    "    for h in hidden_sizes:\n",
    "        for ep in epochs_options:\n",
    "            mse, r2 = run_kfold(lr, h, ep)\n",
    "            print(f\"Learning rate: {lr}, Hidden units: {h}, Epochs: {ep}, MSE: {round(mse,3)}, R^2: {round(r2,3)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It can be seen that the neural network does not capture the underlying relationships within our data, with our R^2 being consistently around 0, and even negative at times. Increasing the number of epochs and hidden units did not improve performance. A neural network is not well suited for this data."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "environments-4MEucMeV",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
