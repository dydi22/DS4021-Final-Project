{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b53dddde",
   "metadata": {},
   "source": [
    "### Test Data Processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e538d40b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "test_df = pd.read_csv('data/test.csv')\n",
    "\n",
    "# drop unnecessary columns\n",
    "test_df = test_df.drop(columns=['weather_temperature', 'weather_wind_mph', 'weather_humidity', 'weather_detail', 'stadium'])\n",
    "test_df.head()\n",
    "\n",
    "# first add total_score column\n",
    "test_df['total_score'] = test_df['score_home'] + test_df['score_away']\n",
    "\n",
    "# add column that represented current record for each team before each game of a season\n",
    "# ensure games are sorted chronologically within each season\n",
    "test_df[\"datetime\"] = pd.to_datetime(test_df[\"schedule_date\"])\n",
    "test_df = test_df.sort_values([\"schedule_season\", \"datetime\"]).reset_index(drop=True)\n",
    "\n",
    "# make output lists\n",
    "home_records = []\n",
    "away_records = []\n",
    "\n",
    "# make dictionaries to track each team's W-L-T within the current season\n",
    "team_wins = {}\n",
    "team_losses = {}\n",
    "team_ties = {}\n",
    "\n",
    "current_season = None\n",
    "\n",
    "for i, row in test_df.iterrows():\n",
    "    season = row[\"schedule_season\"]\n",
    "    home = row[\"team_home\"]\n",
    "    away = row[\"team_away\"]\n",
    "    home_score = row[\"score_home\"]\n",
    "    away_score = row[\"score_away\"]\n",
    "   \n",
    "    # new season, we reset all\n",
    "    if season != current_season:\n",
    "        team_wins = {}\n",
    "        team_losses = {}\n",
    "        team_ties = {}\n",
    "        current_season = season\n",
    "\n",
    "    # initialize teams for this season if needed\n",
    "    for team in [home, away]:\n",
    "        if team not in team_wins:\n",
    "            team_wins[team] = 0\n",
    "            team_losses[team] = 0\n",
    "            team_ties[team] = 0\n",
    "\n",
    "    # add current record before the game\n",
    "    home_records.append(\n",
    "        f\"{team_wins[home]}-{team_losses[home]}-{team_ties[home]}\"\n",
    "    )\n",
    "    away_records.append(\n",
    "        f\"{team_wins[away]}-{team_losses[away]}-{team_ties[away]}\"\n",
    "    )\n",
    "\n",
    "    # update records after the game\n",
    "    if home_score > away_score:\n",
    "        team_wins[home] += 1\n",
    "        team_losses[away] += 1\n",
    "    elif away_score > home_score:\n",
    "        team_wins[away] += 1\n",
    "        team_losses[home] += 1\n",
    "    else:\n",
    "        # tie\n",
    "        team_ties[home] += 1\n",
    "        team_ties[away] += 1\n",
    "\n",
    "# add results to dataframe\n",
    "test_df[\"home_team_record\"] = home_records\n",
    "test_df[\"away_team_record\"] = away_records\n",
    "\n",
    "# make individual columns for wins, losses, and ties\n",
    "test_df['home_wins'] = test_df['home_team_record'].apply(lambda x: int(x.split('-')[0]))\n",
    "test_df['home_losses'] = test_df['home_team_record'].apply(lambda x: int(x.split('-')[1]))\n",
    "test_df['home_ties'] = test_df['home_team_record'].apply(lambda x: int(x.split('-')[2]))\n",
    "test_df['away_wins'] = test_df['away_team_record'].apply(lambda x: int(x.split('-')[0]))\n",
    "test_df['away_losses'] = test_df['away_team_record'].apply(lambda x: int(x.split('-')[1]))\n",
    "test_df['away_ties'] = test_df['away_team_record'].apply(lambda x: int(x.split('-')[2]))\n",
    "\n",
    "# filter games that have already been recorded, no scheduled games\n",
    "test_df = test_df[test_df[\"datetime\"] <= \"2025-11-04\"]\n",
    "\n",
    "# 1. Compute each team's average score per season\n",
    "team_season_avg = (\n",
    "    test_df.groupby([\"team_home\", \"schedule_season\"])[\"score_home\"].mean().reset_index()\n",
    ")\n",
    "team_season_avg.columns = [\"team\", \"season\", \"avg_score\"]\n",
    "\n",
    "# also include away team scoring\n",
    "team_season_avg_away = (\n",
    "    test_df.groupby([\"team_away\", \"schedule_season\"])[\"score_away\"].mean().reset_index()\n",
    ")\n",
    "team_season_avg_away.columns = [\"team\", \"season\", \"avg_score\"]\n",
    "\n",
    "# combine home + away scoring for a true team season average\n",
    "team_season_avg = pd.concat([team_season_avg, team_season_avg_away])\n",
    "team_season_avg = team_season_avg.groupby([\"team\", \"season\"])[\"avg_score\"].mean().reset_index()\n",
    "\n",
    "# 2. Shift averages to represent \"previous season\"\n",
    "team_season_avg[\"prev_season\"] = team_season_avg[\"season\"] + 1\n",
    "\n",
    "# this means: prev_season avg is used in the next year's games\n",
    "team_prev = team_season_avg[[\"team\", \"prev_season\", \"avg_score\"]]\n",
    "team_prev.columns = [\"team\", \"schedule_season\", \"prev_season_avg\"]\n",
    "\n",
    "# 3. Merge into main test_df\n",
    "test_df = test_df.merge(\n",
    "    team_prev,\n",
    "    left_on=[\"team_home\", \"schedule_season\"],\n",
    "    right_on=[\"team\", \"schedule_season\"],\n",
    "    how=\"left\"\n",
    ")\n",
    "test_df.rename(columns={\"prev_season_avg\": \"home_prev_avg\"}, inplace=True)\n",
    "test_df = test_df.drop(columns=[\"team\"])\n",
    "\n",
    "test_df = test_df.merge(\n",
    "    team_prev,\n",
    "    left_on=[\"team_away\", \"schedule_season\"],\n",
    "    right_on=[\"team\", \"schedule_season\"],\n",
    "    how=\"left\"\n",
    ")\n",
    "test_df.rename(columns={\"prev_season_avg\": \"away_prev_avg\"}, inplace=True)\n",
    "test_df = test_df.drop(columns=[\"team\"])\n",
    "\n",
    "home = test_df[[\"schedule_season\", \"datetime\", \"team_home\", \"score_home\", \"score_away\"]].rename(\n",
    "    columns={\"team_home\": \"team\", \"score_home\": \"points_scored\", \"score_away\": \"points_allowed\"}\n",
    ")\n",
    "\n",
    "away = test_df[[\"schedule_season\", \"datetime\", \"team_away\", \"score_away\", \"score_home\"]].rename(\n",
    "    columns={\"team_away\": \"team\", \"score_away\": \"points_scored\", \"score_home\": \"points_allowed\"}\n",
    ")\n",
    "\n",
    "long_df = pd.concat([home, away])\n",
    "long_df = long_df.sort_values([\"team\", \"schedule_season\", \"datetime\"]).reset_index(drop=True)\n",
    "\n",
    "groups = long_df.groupby([\"team\", \"schedule_season\"])\n",
    "\n",
    "long_df[\"rolling_scored\"] = groups[\"points_scored\"].transform(\n",
    "    lambda s: s.shift().expanding().mean()\n",
    ")\n",
    "\n",
    "long_df[\"rolling_allowed\"] = groups[\"points_allowed\"].transform(\n",
    "    lambda s: s.shift().expanding().mean()\n",
    ")\n",
    "\n",
    "test_df = test_df.merge(\n",
    "    long_df[[\"team\", \"schedule_season\", \"datetime\", \"rolling_scored\", \"rolling_allowed\"]],\n",
    "    left_on=[\"team_home\", \"schedule_season\", \"datetime\"],\n",
    "    right_on=[\"team\", \"schedule_season\", \"datetime\"],\n",
    "    how=\"left\"\n",
    ").rename(\n",
    "    columns={\n",
    "        \"rolling_scored\": \"home_rolling_scored\",\n",
    "        \"rolling_allowed\": \"home_rolling_allowed\"\n",
    "    }\n",
    ").drop(columns=[\"team\"])\n",
    "\n",
    "\n",
    "test_df = test_df.merge(\n",
    "    long_df[[\"team\", \"schedule_season\", \"datetime\", \"rolling_scored\", \"rolling_allowed\"]],\n",
    "    left_on=[\"team_away\", \"schedule_season\", \"datetime\"],\n",
    "    right_on=[\"team\", \"schedule_season\", \"datetime\"],\n",
    "    how=\"left\"\n",
    ").rename(\n",
    "    columns={\n",
    "        \"rolling_scored\": \"away_rolling_scored\",\n",
    "        \"rolling_allowed\": \"away_rolling_allowed\"\n",
    "    }\n",
    ").drop(columns=[\"team\"])\n",
    "\n",
    "# drop 2005 due to NaNs from prev season averages\n",
    "test_df = test_df[test_df[\"schedule_season\"] != 2005]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b877152",
   "metadata": {},
   "source": [
    "### Evaluation on Test Set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a3f55934",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.linear_model import Ridge\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "from sklearn.impute import SimpleImputer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "6ee1ffaa",
   "metadata": {},
   "outputs": [],
   "source": [
    "features = [\n",
    "    \"over_under_line\",\n",
    "    \"stadium_neutral\",\n",
    "    \"home_wins\", \"home_losses\", \"home_ties\",\n",
    "    \"away_wins\", \"away_losses\", \"away_ties\",\n",
    "    \"home_prev_avg\", \"away_prev_avg\",\n",
    "    \"home_rolling_scored\", \"home_rolling_allowed\",\n",
    "    \"away_rolling_scored\", \"away_rolling_allowed\"\n",
    "]\n",
    "\n",
    "X_df = test_df[features]\n",
    "X = test_df[features].to_numpy()\n",
    "y = test_df[\"total_score\"].to_numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "27f17bd3",
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline = Pipeline([\n",
    "    (\"imputer\", SimpleImputer(strategy=\"mean\")),  # fills NaNs\n",
    "    (\"scaler\", StandardScaler()),\n",
    "    (\"ridge\", Ridge(alpha=100.0, solver='auto', random_state=123))\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5249bbe4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test MSE:  186.7083\n",
      "Test R2:   0.0763\n"
     ]
    }
   ],
   "source": [
    "# fitting best model to the test data\n",
    "pipeline.fit(X, y)\n",
    "y_pred = pipeline.predict(X)\n",
    "\n",
    "mse  = mean_squared_error(y, y_pred)\n",
    "r2   = r2_score(y, y_pred)\n",
    "\n",
    "print(f\"Test MSE:  {mse:.4f}\")\n",
    "print(f\"Test R2:   {r2:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1fb5a37",
   "metadata": {},
   "source": [
    "Here we see that the R2 was slightly worse with test data than on the validation set from the training data. An R2 of 0.0763 indicates that the model is not a good fit on the data and that there is a lot of unexplained variation."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
